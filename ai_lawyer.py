# -*- coding: utf-8 -*-
"""AI_LAWYER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WZ8Q1rw_BZVfXY7Os2OAhc-BtdHVNwrS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
import keras
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Sequential

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/legal_text_classification.csv")

df.head(5)

plt.figure(figsize = (11, 5))
sns.countplot(x='case_outcome', data=df)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments faiss-cpu

import pandas as pd
import re
import numpy as np
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import faiss

# Clean and combine text
df['case_title'] = df['case_title'].str.strip()
df['case_text'] = df['case_text'].str.strip()

def generate_prompt(row):
    title = row['case_title']
    text = row['case_text']
    return f"""Case Title: {title}

Case Description: {text}

Give a detailed legal verdict including:
- Legal reasoning
- Verdict (Guilty/Not Guilty/Settled/etc.)
- Punishment (if any)
- Relief or Compensation."""

df['prompt'] = df.apply(generate_prompt, axis=1)

# Clean the prompts
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\[\d+\]', '', text)
    return text.strip()

df['prompt'] = df['prompt'].apply(clean_text)

# If 'verdict' column doesn't exist, create a placeholder
if 'verdict' not in df.columns:
    df['verdict'] = [""] * len(df)

# Save cleaned dataset
df.to_csv("preprocessed_ai_lawyer_dataset.csv", index=False)

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(df['prompt'].tolist(), show_progress_bar=True)

# FAISS indexing
embedding_dim = embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(np.array(embeddings))

# Gemini setup
GOOGLE_API_KEY = "************************************"  # ⚠️ Store this securely!
genai.configure(api_key=GOOGLE_API_KEY)
gemini_model = genai.GenerativeModel("gemini-2.0-flash")

# Similar case retrieval
def find_similar_cases(new_prompt, k=3):
    new_vec = model.encode([new_prompt])
    D, I = index.search(np.array(new_vec), k)
    similar_prompts = df.iloc[I[0]]['prompt'].tolist()
    similar_verdicts = df.iloc[I[0]]['verdict'].tolist()
    return list(zip(similar_prompts, similar_verdicts))

# Construct full prompt
def construct_contextual_prompt(new_case_prompt):
    similar_cases = find_similar_cases(new_case_prompt)
    context = ""
    for i, (p, v) in enumerate(similar_cases):
        context += f"\n=== Past Case {i+1} ===\nPrompt:\n{p}\nVerdict:\n{v}\n"
    full_prompt = f"""{context}

New Case:
{new_case_prompt}

Give a detailed legal verdict for this new case using legal reasoning, precedent, and similar outcomes."""
    return full_prompt

# Generate verdict
def get_verdict(new_case_prompt):
    prompt = construct_contextual_prompt(new_case_prompt)
    response = gemini_model.generate_content(prompt)
    return response.text

new_case = """Case Title: your title
Case Description: description of the title"""

verdict = get_verdict(new_case)
print(verdict)